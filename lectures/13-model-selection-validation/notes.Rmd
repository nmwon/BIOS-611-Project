Model Selection and Characterization
====================================

When doing classification, its tempting to do a train/test split,
evaluate the model and then report to whomever that the accuracy of
the model (or precision, or whatever) is X.

Similarly, if we want to try different methods of modelling the same
data (for example, logistic regression vs tree based methods) or we
want to pick the optimal set of features to use as input, we'd probably be 
tempted to 

1. train each model on the train set
2. evaluate each model on the test set
3. choose the model with the best accuracy, f1, etc.

This, of course, doesn't make a lot of sense.

Why? 

We're interested in the _expected_ performance of the model on _new
data_, for which our train/test split only produces a single
estimate. We have not established from just one run what, for example,
the _variance_ of our model parameters are. 

In order to accurately characterize our model (or models, if we are
doing selection) we'd ideally like to repeat the entire modeling
procedure with new samples many times and then estimate the expected
value and variation of the performance metrics we're interest in.

Unfortunately, we rarely have access to such a large amount of data
that this can be done without tricks.

Example:
========

Monday:

```{r}
library(tidyverse);
library(gbm);

data <- read_csv("source_data/wide_power_gender.csv") %>%
    mutate(across(setdiff(everything(),one_of("is_female")), factor));
data
```

We trained a GBM like this:

```{r}


explanatory <- data %>% select(-character, -gender, -is_female) %>% names()
formula <- as.formula(sprintf("is_female ~ %s", paste(explanatory, collapse=" + ")));
tts <- runif(nrow(data)) < 0.5;
train <- data %>% filter(tts);
test <- data %>% filter(!tts);

model <- gbm(formula, data=train);
summary(model, plot=FALSE);

rates <- function(actual, predicted){
    positive_ii <- which(!!actual);
    negative_ii <- which(!actual);

    true_positive <- sum(predicted[positive_ii])/length(positive_ii);
    false_positive <- sum(predicted[negative_ii])/length(negative_ii);

    true_negative <- sum(!predicted[negative_ii])/length(negative_ii);
    false_negative <- sum(!predicted[positive_ii])/length(positive_ii);
    tibble(true_positive=true_positive,
           false_positive=false_positive,
           false_negative=false_negative,
           true_negative=true_negative,
           accuracy=sum(actual==predicted)/length(actual));
}

rates(test$is_female, 1*(predict(model, newdata=test, type="response")>0.5))

```

How can we produce an estimate for the expected values of these
numbers?

K-Fold Cross Validation
=======================

The idea here is to simulate having more data by repeating our train
test split in a very specific way. We are going to "walk down" the
data set in steps of K and re-run our model. This is easy enough for
you to implement yourself:

```{r}

shuffle <- function(a){
    sample(a, length(a), replace=F);
}

k_fold <- function(k, data, formula, model_function, characterization){
    folds <- shuffle(floor(k*(1:nrow(data)-1)/nrow(data))+1);
    fold_size <- mean(table(folds) %>% as.numeric());
    print(sprintf("Average fold size %f", fold_size))
    do.call(rbind, Map(function(k){
        train <- data %>% filter(folds != k);
        test <- data %>% filter(folds == k);
        model <- model_function(formula, train);
        characterization(model, test);
    }, seq(k)));
}

results <- k_fold(20, data, formula,
                  function(f, d){
                      gbm(f, data=d);
                  },
                  function(m, test){
                      rates(test$is_female,
                            1*(predict(m, newdata=test, type="response")>0.5));
                  });

dumb_accuracy <- sum(data$is_female == 0)/nrow(data);

ggplot(results, aes(accuracy)) + geom_density() +
    geom_segment(aes(x=accuracy, xend=accuracy, y=0, yend=0.5)) +
    geom_segment(x=dumb_accuracy, xend=dumb_accuracy, y=0, yend=7, color="red");

```

What this tells us is that (assuming we have a reliable estimate for
the proportion of male and female characters) we can reasonably say
that a randomly chosen model has about a 40% chance of being _less
accurate_ than the dumbest model (guessing "male" all the time).

Bias Variance Tradeoff
======================

There is a trade off between variance and bias when doing cross
validation with k-folds.

The large the number k, the larger your variance in the estimate of
your result but the smaller the bias.

Vs Bootstrapping
================

You may also encounter bootstrapping as a method of characterizing a
model. In this method, you train on a sample (with replacement) from
your data and test on the data set of those data points not in the
bootstrap.

Bootstrapping isn't as commonly used as a model characterization step,
but turns out to be useful in various tree based methods of regression
and calculation.

Parameter Tuning
================

Our gradient boosting machine/Adaboost classifier actually has some
tunable parameters which we've ignored so far. These are so-called
"hyperparameters" because they are knobs you have on the model which
you can tune by hand but are not tuned by the training process.

We'd often like to choose optimal values for the hyperparameters.

Now you know how you might do this in practice: select pair of
hyperparameter settings, characterize the models, estimate the mean
performance of each model using k-fold cross validation, and select
the better of the two sets of hyperparameters.

This is doable with plain R but libraries exist to do the work for us.

Enter Caret
===========

The Caret library is a very extensive package for model
characterization and selection.

The upside of using Caret is that if your model type is supported it
will do all the work for you. The downside is that the interpretation
of the results is a little trick and it can be a little harder to work
with than simply running your model once.

Example: Tuning our GBM
=======================

Our GBM has a few tunable parameters. 

1. n.trees - the number of different tress that "vote" on our instances
2. interaction.depth - how many decisions in each tree
3. shrinkage - a parameter that has to do with how each step of the
   algorithm modifies the prediction of the model

```{r}
library(caret);

trainIndex <- createDataPartition(data$is_female, p = .8, 
                                  list = FALSE, 
                                  times = 1)

egdata <- data;

egdata$is_female <- factor(egdata$is_female);

train_ctrl <- trainControl(method = "cv", number = 50);
gbmFit1 <- train(formula, data = egdata %>% slice(trainIndex), 
                 method = "gbm", 
                 trControl = train_ctrl,
                 ## This last option is actually one
                 ## for gbm() that passes through
                 verbose = FALSE)
summary(gbmFit1);
gbmFit1
```

The result here is a table of hyperparameters and their accuracy and
kappa values.

(NB: kappa: 
$$
\kappa = \frac{2 \times (TP \times TN - FN \times FP)}{(TP + FP) \times (FP + TN) + (TP + FN) \times (FN + TN)}
$$

"Cohen's kappa measures the agreement between two raters who each classify N items into C mutually exclusive categories...  If the raters are in complete agreement then κ = 1. If there is no agreement among the raters other than what would be expected by chance (as given by pe), κ = 0. It is possible for the statistic to be negative which implies that there is no effective agreement between the two raters or the agreement is worse than random."

[Cohen's kappa Wikipedia Article](https://en.wikipedia.org/wiki/Cohen%27s_kappa))

Moral
=====

If you are using a complicated model, you probably want to use Caret
to tune the hyperparameters. At least think about it.

Deeper moral: if you are at the point of fine tuning parameters,
you're model situation probably doesn't work that well anyway!

Feature Selection
=================

Consider our model summary:

```{r}
summary(gbmFit1);
```

Maybe we can improve our model by using a subset of these columns?

We could just try it:

```{r}
smaller_formula <- is_female ~ unique_physiology + immortality + superhuman_agility + flight;
smodel <- gbm(smaller_formula, data=train, n.trees=100, interaction.depth=2);

rates(test$is_female, 1*(predict(smodel, newdata=test, type="response")>0.5))

```

But there is a pretty big exponential explosion in trying to do this
kind of selection. There are various strategies to tame this complexity.

Recursive Feature Elimination
=============================

The basic approach is to 

1. Fit a model with N variables
2. a. Find the parameter with the least strength
   b. characterize the model performance
3. Remove that one, go back to 1

Show the results.

We don't have to do this by hand, but using RFE with Caret for GBM
models is a bit of a hassle.

We need to implement an API.

```{r}
gbmFit <- function(x, y, first, last, ...){
    df <- x %>% as_tibble() %>% mutate(to_predict=y);
    formula <- as.formula(sprintf("to_predict ~ %s",paste(names(x),collapse=" + ")));
    model <- gbm(formula, distribution="bernoulli", data=df, ...);
    model
}
gbmPred <- function(model, x){
    1*(predict(model, newdata=x, type="response") > 0.5);
}
gbmRank <- function(model, x, y){
    summary(model) %>% as_tibble() %>% rename(Overall=rel.inf);
}

gbmFuncs <- list(
    fit=gbmFit,
    pred=gbmPred,
    rank=gbmRank,
    selectSize=function (x, metric, maximize) 
    {
        best <- if (maximize) 
                    which.max(x[, metric])
                else which.min(x[, metric])
        min(x[best, "Variables"])
    },
    selectVar=function (y, size) 
    {
        print(size)
        finalImp <- plyr::ddply(y[, c("Overall", "var")], plyr::.(var), function(x) mean(x$Overall, 
                                                                             na.rm = TRUE))
        names(finalImp)[2] <- "Overall"
        finalImp <- finalImp[order(finalImp$Overall, decreasing = TRUE), 
                             ]
        as.character(finalImp$var[1:size])
    },
    summary=function(data, lev=NULL, model=NULL){
        c("Accuracy"=sum(data$pred==data$obs)/length(data$pred));
    }
);

ctrl <- rfeControl(functions=gbmFuncs,
                   method="repeatedcv",
                   repeats=5,
                   verbose=F);

results <- rfe(data %>% select(all_of(explanatory)), data %>% pull(is_female),
               sizes=3:21,
               rfeControl=ctrl,
               metric="Accuracy");
results

```

Gee - what a lot of work to find out that the best model is is the one
with most 21.

Comparing Multiple Models
=========================

So far we've looked at two sorts of model comparisons: hyper-parameter
tuning and variable selection.

After we've tuned our model this way, we may want to compare it to
another model altogether based on some criteria.

This is the reason you often see a "train, validation and test" split
in the machine literature.

If you plan on doing multiple model comparisons, the most virtuous
thing to do is set aside as large of a subset of data as you can as
your "test" set. Then you can use k-fold cross validation or other
strategies to characterize your distinct models before finally testing
their performance on your held out testing set.

Concluding Notes
================

The Caret package is kind of the Sherman Tank of model validation and
selection. In an ideal situation your model will have good performance
with default parameters and/or a simple model will
suffice. Frequently, if your model doesn't perform well on its initial
pass, then different models and/or parameters won't make much of a
difference.

Still, in some situations you want to run down the very best model you
can. 
